{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhcwPQ1Crxc8MjHR3Ib/Vd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","\n","# Given values for the 4-neuron network\n","weights = np.array(\n","    [\n","        [1, -1, 1],  # Neuron 1 weights\n","        [1, 1, 0],  # Neuron 2 weights\n","        [0, 1, 1],  # Neuron 3 weights\n","        [1, 0, 1],  # Neuron 4 weights\n","    ]\n",")\n","biases = np.array([-5, 0, 1, -2])\n","inputs = np.array([2, 1, 3])\n","\n","# Define the ReLU activation function for array inputs\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","\n","# Function to compute the output of a network with 4 neurons\n","def four_neuron_network(weights, biases, inputs):\n","\n","    outputs_before_activation = []\n","    activated_outputs = []\n","\n","    for i, (weight, bias) in enumerate(zip(weights, biases), start=1):\n","        # Calculate the neuron's outputs before activation\n","        output_before_activation = np.dot(weight, inputs) + bias\n","\n","        # Apply the ReLU activation function\n","        activated_output = relu(output_before_activation)\n","\n","        # Add the calculation step for each neuron\n","        calculation_steps = [f\"({w})*({x})\" for w, x in zip(weight, inputs)]\n","        calculation_steps.append(f\"({bias})\")\n","        output_text = f\"w{i} * x + b{i} = \" + \" + \".join(calculation_steps)\n","        output_text += f\" = {output_before_activation}\\n\"\n","\n","        outputs_before_activation.append(output_before_activation)\n","        activated_outputs.append(activated_output)\n","\n","    # Count the parameters of each node: number of weights + 1 bias\n","    params_per_node = weights.shape[1] + 1\n","\n","    # Count all the parameters of this network\n","    total_params = weights.shape[0] * params_per_node\n","\n","    print(output_text)\n","    return outputs_before_activation, activated_outputs, params_per_node, total_params\n","\n","\n","# Perform the operation\n","outputs_before_activation, activated_outputs, params_per_node, total_params = (\n","    four_neuron_network(weights, biases, inputs)\n",")\n","\n","# Print the outcomes\n","print(\"Outputs before ReLU activation:\\n\", outputs_before_activation)\n","print(\n","    \"Outputs after ReLU activation: (negative values → 0; positive values → same values)\\n\",\n","    activated_outputs,\n",")\n","print(\"Parameters per node: (number of weights + 1 bias)\\n\", params_per_node)\n","print(\"Total parameters in the network:\\n\", total_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ng-PiILwt0S1","executionInfo":{"status":"ok","timestamp":1710601867075,"user_tz":300,"elapsed":260,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"b1eb80bc-b6c6-4f74-ea67-a588710ae88a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["w4 * x + b4 = (1)*(2) + (0)*(1) + (1)*(3) + (-2) = 3\n","\n","Outputs before ReLU activation:\n"," [-1, 3, 5, 3]\n","Outputs after ReLU activation: (negative values → 0; positive values → same values)\n"," [0, 3, 5, 3]\n","Parameters per node: (number of weights + 1 bias)\n"," 4\n","Total parameters in the network:\n"," 16\n"]}]}]}