{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWNkCGOxkHtRDZYd4KPDtm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Implementación de Red Neuronal para clasificación con MLPClassifier\n","\n","---"],"metadata":{"id":"M_C6IO6O57qX"}},{"cell_type":"markdown","source":["**1. Cargamos las librerías**"],"metadata":{"id":"Y6UJAxlWy0mO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKPND5Aawlko"},"outputs":[],"source":["# Cargamos las librerías a usar\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","source":["**2. Carga y exploración de datos**"],"metadata":{"id":"-goKiOcEy-nw"}},{"cell_type":"code","source":["# Carga el conjunto de datos de Iris\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Explorando los datos\n","type(iris)\n","iris.keys()\n","iris['data']\n","iris['target']\n","iris['target_names']\n","iris['DESCR']\n","iris['feature_names']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhDXZxjczCEY","executionInfo":{"status":"ok","timestamp":1710551507216,"user_tz":300,"elapsed":175,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"cbab6bca-bd8b-4a89-b8bd-60deb6090e9c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['sepal length (cm)',\n"," 'sepal width (cm)',\n"," 'petal length (cm)',\n"," 'petal width (cm)']"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["**3. Dividimos los datos en conjuntos de entrenamiento y prueba**"],"metadata":{"id":"Pa-lTv-A2aDa"}},{"cell_type":"code","source":["# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"],"metadata":{"id":"h97rc7Kt0PpQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. Escalamos las características utilizando StandardScaler para asegurarnos de que todas tengan la misma escala**"],"metadata":{"id":"0ctUsCYs2vN0"}},{"cell_type":"code","source":["# Escalas las características para un mejor rendimiento del modelo\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n"],"metadata":{"id":"XcE1jAhx1Jku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**5. Creamos una instancia de MLPClassifier con una capa oculta de 100 neuronas, función de activación ReLU, algoritmo de optimización 'adam' y un máximo de 500 iteraciones.**"],"metadata":{"id":"AQ1orpig3oVL"}},{"cell_type":"code","source":["# Crear uns instancia MLPClassifier\n","mlp_clf = MLPClassifier(hidden_layer_sizes=(100),\n","                        activation = 'relu',\n","                        solver = 'adam',\n","                        max_iter = 100,\n","                        random_state=42,\n","                        verbose=True)"],"metadata":{"id":"AUqCZesY3mls"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**6. Entrenamos el modelo utilizando los datos de entrenamiento escalados.**"],"metadata":{"id":"tXFBiTIv4Sr8"}},{"cell_type":"code","source":["# Entrenar el modelo\n","mlp_clf.fit(X_train_scaled, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DMjl0Mlx4ens","executionInfo":{"status":"ok","timestamp":1710552724940,"user_tz":300,"elapsed":196,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"3e8c796e-0f1c-4f8e-fb28-02ac1cfdbd5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 1.16374923\n","Iteration 2, loss = 1.14090611\n","Iteration 3, loss = 1.11857816\n","Iteration 4, loss = 1.09677375\n","Iteration 5, loss = 1.07547149\n","Iteration 6, loss = 1.05468504\n","Iteration 7, loss = 1.03440573\n","Iteration 8, loss = 1.01462373\n","Iteration 9, loss = 0.99535820\n","Iteration 10, loss = 0.97658926\n","Iteration 11, loss = 0.95833257\n","Iteration 12, loss = 0.94053852\n","Iteration 13, loss = 0.92322169\n","Iteration 14, loss = 0.90638373\n","Iteration 15, loss = 0.89000670\n","Iteration 16, loss = 0.87408395\n","Iteration 17, loss = 0.85861030\n","Iteration 18, loss = 0.84357120\n","Iteration 19, loss = 0.82895431\n","Iteration 20, loss = 0.81476466\n","Iteration 21, loss = 0.80097410\n","Iteration 22, loss = 0.78759157\n","Iteration 23, loss = 0.77460302\n","Iteration 24, loss = 0.76199129\n","Iteration 25, loss = 0.74975377\n","Iteration 26, loss = 0.73786826\n","Iteration 27, loss = 0.72632581\n","Iteration 28, loss = 0.71511871\n","Iteration 29, loss = 0.70424319\n","Iteration 30, loss = 0.69368183\n","Iteration 31, loss = 0.68342580\n","Iteration 32, loss = 0.67346953\n","Iteration 33, loss = 0.66380608\n","Iteration 34, loss = 0.65442161\n","Iteration 35, loss = 0.64531278\n","Iteration 36, loss = 0.63647902\n","Iteration 37, loss = 0.62790342\n","Iteration 38, loss = 0.61957863\n","Iteration 39, loss = 0.61148527\n","Iteration 40, loss = 0.60361466\n","Iteration 41, loss = 0.59597076\n","Iteration 42, loss = 0.58855743\n","Iteration 43, loss = 0.58136358\n","Iteration 44, loss = 0.57437150\n","Iteration 45, loss = 0.56758016\n","Iteration 46, loss = 0.56097952\n","Iteration 47, loss = 0.55455113\n","Iteration 48, loss = 0.54829175\n","Iteration 49, loss = 0.54219766\n","Iteration 50, loss = 0.53626159\n","Iteration 51, loss = 0.53048210\n","Iteration 52, loss = 0.52484566\n","Iteration 53, loss = 0.51933866\n","Iteration 54, loss = 0.51394846\n","Iteration 55, loss = 0.50869354\n","Iteration 56, loss = 0.50356710\n","Iteration 57, loss = 0.49857160\n","Iteration 58, loss = 0.49370008\n","Iteration 59, loss = 0.48894487\n","Iteration 60, loss = 0.48429908\n","Iteration 61, loss = 0.47974886\n","Iteration 62, loss = 0.47529331\n","Iteration 63, loss = 0.47093384\n","Iteration 64, loss = 0.46667067\n","Iteration 65, loss = 0.46249978\n","Iteration 66, loss = 0.45841914\n","Iteration 67, loss = 0.45442291\n","Iteration 68, loss = 0.45050855\n","Iteration 69, loss = 0.44667746\n","Iteration 70, loss = 0.44292801\n","Iteration 71, loss = 0.43926405\n","Iteration 72, loss = 0.43567120\n","Iteration 73, loss = 0.43215951\n","Iteration 74, loss = 0.42871948\n","Iteration 75, loss = 0.42534205\n","Iteration 76, loss = 0.42201917\n","Iteration 77, loss = 0.41875279\n","Iteration 78, loss = 0.41554234\n","Iteration 79, loss = 0.41238463\n","Iteration 80, loss = 0.40928512\n","Iteration 81, loss = 0.40623752\n","Iteration 82, loss = 0.40323632\n","Iteration 83, loss = 0.40027444\n","Iteration 84, loss = 0.39735268\n","Iteration 85, loss = 0.39447038\n","Iteration 86, loss = 0.39162936\n","Iteration 87, loss = 0.38882549\n","Iteration 88, loss = 0.38605547\n","Iteration 89, loss = 0.38331954\n","Iteration 90, loss = 0.38062197\n","Iteration 91, loss = 0.37796519\n","Iteration 92, loss = 0.37534717\n","Iteration 93, loss = 0.37276083\n","Iteration 94, loss = 0.37020149\n","Iteration 95, loss = 0.36766960\n","Iteration 96, loss = 0.36516474\n","Iteration 97, loss = 0.36267665\n","Iteration 98, loss = 0.36021595\n","Iteration 99, loss = 0.35777846\n","Iteration 100, loss = 0.35536449\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(hidden_layer_sizes=100, max_iter=100, random_state=42,\n","              verbose=True)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=100, random_state=42,\n","              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=100, random_state=42,\n","              verbose=True)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["**7. Realizamos predicciones en el conjunto de prueba.**"],"metadata":{"id":"TKxdjo1G45ZL"}},{"cell_type":"code","source":["y_pred = mlp_clf.predict(X_test_scaled)\n","print(y_test)\n","print(y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNcKoKI14_6z","executionInfo":{"status":"ok","timestamp":1710552828976,"user_tz":300,"elapsed":166,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"8f17e6ee-f434-47fb-dcd1-94f6d6e9622f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n","[1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"]}]},{"cell_type":"markdown","source":["**8. Calculamos la precisión del modelo utilizando la función accuracy_score**"],"metadata":{"id":"rLWUq9jm5RW6"}},{"cell_type":"code","source":["accuracy = accuracy_score(y_test, y_pred)\n","print(\"Precisión del modelo\", round(accuracy,2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnpUIAfA5ZBx","executionInfo":{"status":"ok","timestamp":1710552945745,"user_tz":300,"elapsed":195,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"0ecc0f62-80cf-40e8-df81-80478b884f90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precisión del modelo 0.93\n"]}]},{"cell_type":"markdown","source":["# Implementación de Red Neuronal para regresión utilizando MLPRegressor\n","\n","---"],"metadata":{"id":"q8bXFjQZ6jx2"}},{"cell_type":"code","source":["# Cargamos las librerías a usar\n","\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error"],"metadata":{"id":"ObC2fwSY6vQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cargar el conjunto de datos de california Housing\n","housing = fetch_california_housing()\n","X, y = housing.data, housing.target\n","\n","# Explorando los datos\n","type(housing)\n","housing.keys()\n","housing['data']\n","housing['target']\n","housing['target_names']\n","housing['DESCR']\n","housing['feature_names']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ouNtR8kr6-02","executionInfo":{"status":"ok","timestamp":1710553513857,"user_tz":300,"elapsed":1498,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"9f4b6de5-1fdd-4510-ce91-8ab1240d1a6e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['MedInc',\n"," 'HouseAge',\n"," 'AveRooms',\n"," 'AveBedrms',\n"," 'Population',\n"," 'AveOccup',\n"," 'Latitude',\n"," 'Longitude']"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"],"metadata":{"id":"BgXLjsXE7ooG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Escalar las características para un mejor rendimiento del modelo\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"],"metadata":{"id":"DRPWcc9i72Os"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear uns instancia MLPClassifier\n","mlp_reg = MLPRegressor(hidden_layer_sizes=(100),\n","                        activation = 'relu',\n","                        solver = 'adam',\n","                        max_iter = 100,\n","                        random_state=42,\n","                        verbose=True)"],"metadata":{"id":"Hsxu7J8c8BUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrenar el modelo\n","mlp_reg.fit(X_train_scaled, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"coL4n1KU8Tn1","executionInfo":{"status":"ok","timestamp":1710553660651,"user_tz":300,"elapsed":5270,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"5a00608e-6b5b-4651-c865-7d5a7f829734"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 1.62056447\n","Iteration 2, loss = 0.43678846\n","Iteration 3, loss = 0.32700527\n","Iteration 4, loss = 0.28069940\n","Iteration 5, loss = 0.24814135\n","Iteration 6, loss = 0.22803342\n","Iteration 7, loss = 0.21538976\n","Iteration 8, loss = 0.20787968\n","Iteration 9, loss = 0.20154389\n","Iteration 10, loss = 0.19743378\n","Iteration 11, loss = 0.19321222\n","Iteration 12, loss = 0.18961729\n","Iteration 13, loss = 0.18678600\n","Iteration 14, loss = 0.18383630\n","Iteration 15, loss = 0.18157601\n","Iteration 16, loss = 0.17968086\n","Iteration 17, loss = 0.17775103\n","Iteration 18, loss = 0.17538371\n","Iteration 19, loss = 0.17423819\n","Iteration 20, loss = 0.17407294\n","Iteration 21, loss = 0.17085613\n","Iteration 22, loss = 0.16995187\n","Iteration 23, loss = 0.16823352\n","Iteration 24, loss = 0.16798530\n","Iteration 25, loss = 0.16717835\n","Iteration 26, loss = 0.16566984\n","Iteration 27, loss = 0.16730781\n","Iteration 28, loss = 0.16499618\n","Iteration 29, loss = 0.16261953\n","Iteration 30, loss = 0.16145474\n","Iteration 31, loss = 0.16053131\n","Iteration 32, loss = 0.16002333\n","Iteration 33, loss = 0.16125255\n","Iteration 34, loss = 0.15863582\n","Iteration 35, loss = 0.15835131\n","Iteration 36, loss = 0.15795060\n","Iteration 37, loss = 0.15703766\n","Iteration 38, loss = 0.15707574\n","Iteration 39, loss = 0.15768516\n","Iteration 40, loss = 0.15525691\n","Iteration 41, loss = 0.15632172\n","Iteration 42, loss = 0.15467901\n","Iteration 43, loss = 0.15438915\n","Iteration 44, loss = 0.15556876\n","Iteration 45, loss = 0.15329744\n","Iteration 46, loss = 0.15273177\n","Iteration 47, loss = 0.15307344\n","Iteration 48, loss = 0.15369159\n","Iteration 49, loss = 0.15240098\n","Iteration 50, loss = 0.15143071\n","Iteration 51, loss = 0.15112755\n","Iteration 52, loss = 0.15083169\n","Iteration 53, loss = 0.15061975\n","Iteration 54, loss = 0.17536085\n","Iteration 55, loss = 0.15573187\n","Iteration 56, loss = 0.15049969\n","Iteration 57, loss = 0.14988656\n","Iteration 58, loss = 0.15022315\n","Iteration 59, loss = 0.14961153\n","Iteration 60, loss = 0.14848394\n","Iteration 61, loss = 0.14950369\n","Iteration 62, loss = 0.14843954\n","Iteration 63, loss = 0.14811978\n","Iteration 64, loss = 0.14766317\n","Iteration 65, loss = 0.14793107\n","Iteration 66, loss = 0.14733359\n","Iteration 67, loss = 0.14800560\n","Iteration 68, loss = 0.14665149\n","Iteration 69, loss = 0.14686777\n","Iteration 70, loss = 0.14665771\n","Iteration 71, loss = 0.14612037\n","Iteration 72, loss = 0.14639346\n","Iteration 73, loss = 0.14671723\n","Iteration 74, loss = 0.14576906\n","Iteration 75, loss = 0.15060883\n","Iteration 76, loss = 0.14555955\n","Iteration 77, loss = 0.14514246\n","Iteration 78, loss = 0.14553115\n","Iteration 79, loss = 0.14643439\n","Iteration 80, loss = 0.14501085\n","Iteration 81, loss = 0.14468993\n","Iteration 82, loss = 0.14489975\n","Iteration 83, loss = 0.14516740\n","Iteration 84, loss = 0.14435448\n","Iteration 85, loss = 0.14397719\n","Iteration 86, loss = 0.14633622\n","Iteration 87, loss = 0.14389402\n","Iteration 88, loss = 0.14379916\n","Iteration 89, loss = 0.14337806\n","Iteration 90, loss = 0.14402555\n","Iteration 91, loss = 0.14472118\n","Iteration 92, loss = 0.14661691\n","Iteration 93, loss = 0.15462744\n","Iteration 94, loss = 0.14386535\n","Iteration 95, loss = 0.14275879\n","Iteration 96, loss = 0.14295641\n","Iteration 97, loss = 0.14219076\n","Iteration 98, loss = 0.14180756\n","Iteration 99, loss = 0.14206889\n","Iteration 100, loss = 0.14286992\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPRegressor(hidden_layer_sizes=100, max_iter=100, random_state=42,\n","             verbose=True)"],"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=100, max_iter=100, random_state=42,\n","             verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=100, max_iter=100, random_state=42,\n","             verbose=True)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# Realizar predicciones en el conjunto de prueba\n","y_pred = mlp_reg.predict(X_test_scaled)"],"metadata":{"id":"0wCUNjCb8ZRz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calcular el error cuadrático medio del modelo\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"Error cuadrático medio del modelo: \", round(mse,3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0OCQyUg8o5v","executionInfo":{"status":"ok","timestamp":1710553803017,"user_tz":300,"elapsed":187,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"0d8716cd-4456-4e48-d94a-e629bed913f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error cuadrático medio del modelo:  0.329\n"]}]},{"cell_type":"code","source":["error = y_test - y_pred\n","print(error)\n","error"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QeaQhMST-b1i","executionInfo":{"status":"ok","timestamp":1710554222312,"user_tz":300,"elapsed":185,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"6b37e2c8-014a-47ad-86a7-3cf9cc576719"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.11614674 -0.78451853  0.35386598 ...  0.18160072 -0.07049547\n"," -0.20138212]\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 0.11614674, -0.78451853,  0.35386598, ...,  0.18160072,\n","       -0.07049547, -0.20138212])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["lista = [valor**2  for valor in error]\n","lista"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ak6GrICa_C-c","executionInfo":{"status":"ok","timestamp":1710554508728,"user_tz":300,"elapsed":201,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"dfffa4e3-2f94-4f6d-adca-82a1e96255da"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.01349006470162435,\n"," 0.6154693184540858,\n"," 0.12522113302122298,\n"," 0.1732655838184803,\n"," 0.012687375276175736,\n"," 0.002971963237614876,\n"," 0.40170324812355457,\n"," 0.02260041452952433,\n"," 0.6731209340076371,\n"," 1.0523202908625065e-06,\n"," 0.02745796159451556,\n"," 0.34219640195926515,\n"," 0.5092845486763115,\n"," 0.15214947632715303,\n"," 0.09666400273974009,\n"," 0.03857850103654245,\n"," 0.0810354068612068,\n"," 0.10851625242260733,\n"," 0.24328617355833013,\n"," 0.07725156519875023,\n"," 0.05686883095923089,\n"," 0.24995787968700894,\n"," 0.013928759406108625,\n"," 1.2394544918884165,\n"," 0.01658990444476626,\n"," 0.002390853580607302,\n"," 0.4674323596805431,\n"," 0.18643739835071382,\n"," 0.04275905666285562,\n"," 0.18458846035603843,\n"," 0.4620028846543583,\n"," 0.004796594488874464,\n"," 0.268699352082754,\n"," 1.816785844551476,\n"," 0.3651717287279569,\n"," 0.024610209219529004,\n"," 0.30399865863862663,\n"," 2.911526089022133e-06,\n"," 0.013617083398532029,\n"," 0.021064004814224876,\n"," 0.07485261757892558,\n"," 0.0029710373398375055,\n"," 0.17896371442906345,\n"," 8.528537590156832e-05,\n"," 0.00790323358598194,\n"," 0.8249931565175431,\n"," 0.004393140106764427,\n"," 0.9887440779535348,\n"," 0.07829835495519896,\n"," 0.06474018630722138,\n"," 0.0038043935987590416,\n"," 0.21516843541610586,\n"," 0.024605995614696004,\n"," 0.009986736549787222,\n"," 0.0016909126430467928,\n"," 0.04861203102544512,\n"," 0.037627254217371606,\n"," 0.0021332492691733844,\n"," 0.24544817755874218,\n"," 6.274366044075393e-08,\n"," 2.4154810358236416,\n"," 0.019185607175400597,\n"," 0.05730131474952183,\n"," 0.0010634293303356,\n"," 0.3390996026410634,\n"," 0.11472800797015059,\n"," 1.2532710449056037,\n"," 0.09246440129404347,\n"," 0.23053141662872742,\n"," 0.010367209575933909,\n"," 0.21080099234168198,\n"," 0.4248597403651966,\n"," 0.04455815144113199,\n"," 0.000671540109327134,\n"," 0.0008639198305187384,\n"," 0.5129724580188937,\n"," 0.06702967078238445,\n"," 0.01155961382979271,\n"," 0.0317623978923217,\n"," 1.3823268265463908,\n"," 0.02217426122677948,\n"," 0.07451948068796745,\n"," 0.03286345924407936,\n"," 0.35389957687731677,\n"," 0.030820860899267135,\n"," 0.4349869657634175,\n"," 0.031653244808601495,\n"," 0.11591876321471514,\n"," 0.14384300395446925,\n"," 0.07679453494745324,\n"," 0.2668987897164041,\n"," 0.002012486627329954,\n"," 0.012249602219376274,\n"," 7.420448262647468e-05,\n"," 0.04684434590444051,\n"," 0.009350967413054457,\n"," 1.1877598322432976,\n"," 0.011861469358055978,\n"," 2.0176714996190785,\n"," 0.08610405818591181,\n"," 0.41114668611141597,\n"," 0.24808537629027344,\n"," 3.244398522685924,\n"," 0.004053749073563259,\n"," 0.23454087192802325,\n"," 0.014348532275497206,\n"," 0.0017217164401699939,\n"," 0.19989171230128214,\n"," 0.015164658093550394,\n"," 0.12312321048456698,\n"," 0.011605005219211137,\n"," 0.032634051815133874,\n"," 6.643348379298692,\n"," 1.0851026714129837,\n"," 0.025995944256657577,\n"," 0.5330441390782829,\n"," 0.0578522963857152,\n"," 0.03647345846922125,\n"," 0.00041525298294194633,\n"," 0.4031416069542936,\n"," 0.012121786517651743,\n"," 0.5142171563190149,\n"," 0.29857713521476453,\n"," 0.11749487495426632,\n"," 0.001548405224542477,\n"," 0.0007194499424501504,\n"," 0.03368505054228332,\n"," 0.013151059429891787,\n"," 0.2022335755647353,\n"," 0.029550865219670656,\n"," 0.6076063017846859,\n"," 0.40441898689743744,\n"," 0.0032458898669325284,\n"," 0.023346103377603598,\n"," 0.017031801332557727,\n"," 0.495252936208151,\n"," 0.7001474035989199,\n"," 0.21907042250692718,\n"," 0.5564311033358594,\n"," 1.0782553978475873,\n"," 3.66103943379418,\n"," 0.007535833277181303,\n"," 0.08629510824406103,\n"," 0.06951774335062605,\n"," 0.5885945854236324,\n"," 0.0016616700518732699,\n"," 0.7150019161693122,\n"," 0.03219719999082189,\n"," 0.013958553295153067,\n"," 0.04015847349355503,\n"," 0.004448326991081618,\n"," 0.03844846376882509,\n"," 0.020248373499361642,\n"," 0.00032183600423743553,\n"," 0.1395540693549047,\n"," 0.022587253318005988,\n"," 0.40423049967195596,\n"," 0.026613338925060914,\n"," 0.07263954447976621,\n"," 0.19173445916870244,\n"," 1.2048466038199916,\n"," 0.0022902490672010597,\n"," 0.04464284436692795,\n"," 0.49674408115678254,\n"," 0.0672311588252928,\n"," 0.11328909564472574,\n"," 0.1630279093923491,\n"," 0.03611421734042829,\n"," 0.15063516736057395,\n"," 0.05074665805553738,\n"," 0.0010291491552119165,\n"," 0.022276766921373602,\n"," 0.2024110997091267,\n"," 1.674834545500392e-05,\n"," 0.044742950398880765,\n"," 0.023263377534686124,\n"," 0.7623244327846084,\n"," 0.37903499043740063,\n"," 0.000792474724370274,\n"," 0.02245588596908729,\n"," 0.004933533341730654,\n"," 1.5234488554293357,\n"," 0.02085078934038359,\n"," 0.07260006608483235,\n"," 0.03165648097427843,\n"," 0.18930373414741977,\n"," 0.00032846404919346705,\n"," 0.15184771870435426,\n"," 0.06268330062925313,\n"," 0.22522375563693514,\n"," 0.03164280679081804,\n"," 0.1469694270549577,\n"," 0.010087099120223526,\n"," 0.18112152529980202,\n"," 5.137815749526943,\n"," 0.007302178872545534,\n"," 0.0013113838988299217,\n"," 0.0005572904766617256,\n"," 0.01888461453770749,\n"," 0.3123194164903011,\n"," 0.00024367499424783205,\n"," 0.08357291338539985,\n"," 0.10722656919243692,\n"," 0.37722771648111086,\n"," 0.13423987415442756,\n"," 0.23680988395916716,\n"," 0.0017024610800374998,\n"," 0.1863262349357456,\n"," 0.00958742974903048,\n"," 0.0977012053763484,\n"," 0.011145422313957163,\n"," 0.02405181497457611,\n"," 0.04565001135529438,\n"," 0.03560198604486958,\n"," 0.3446432626998603,\n"," 0.00024617566295848906,\n"," 0.016871016696256604,\n"," 0.030085356773948916,\n"," 0.48615670780028425,\n"," 0.0735245540053897,\n"," 0.4245613253743669,\n"," 0.013317000704610452,\n"," 0.5009038985887426,\n"," 1.1173614684107112,\n"," 0.3146753733373587,\n"," 1.0848824900901102,\n"," 0.0063172566664601295,\n"," 0.032428111281072765,\n"," 0.004149353294346267,\n"," 0.08367657413900724,\n"," 0.10194239066700819,\n"," 0.6851138390158097,\n"," 0.1494132674205569,\n"," 0.28051689142251063,\n"," 0.16478794725166446,\n"," 0.006734315556077425,\n"," 0.01797985446203476,\n"," 0.2048830389659339,\n"," 0.24758292185680333,\n"," 0.12167008469631498,\n"," 0.1585013535055426,\n"," 0.00551891476302499,\n"," 0.007488054722207186,\n"," 0.0181623080114229,\n"," 0.3580950585188372,\n"," 0.005747785246219246,\n"," 1.4580605243437572,\n"," 0.3603087554179367,\n"," 0.575448939095645,\n"," 0.0312708025070151,\n"," 1.442203381567172,\n"," 0.006030916983842749,\n"," 5.892820008763944e-05,\n"," 0.052439168956364954,\n"," 0.03713865852476683,\n"," 0.11165289517095194,\n"," 0.13808691373989734,\n"," 0.06195550174963324,\n"," 0.11018220253897744,\n"," 0.015163824612742988,\n"," 1.356438279192686,\n"," 0.0020687582027225137,\n"," 0.29693078830806574,\n"," 0.009149335700954175,\n"," 0.2595991500011102,\n"," 1.4004575472607077,\n"," 0.16147962174474667,\n"," 0.004574477029329402,\n"," 0.04223377153214071,\n"," 0.2686218532647751,\n"," 0.0020564982283967034,\n"," 0.4176298617101764,\n"," 0.05533583127362966,\n"," 0.0011159345133350655,\n"," 0.011687489265127398,\n"," 3.7032986949527937,\n"," 0.48767797549268516,\n"," 0.04105738726616507,\n"," 0.011021538861919802,\n"," 0.2705539368237522,\n"," 0.17973003377420166,\n"," 0.17061477221900131,\n"," 0.14806835865041804,\n"," 0.2701121285995958,\n"," 0.2526135259449982,\n"," 9.823011553927863,\n"," 0.0006615213155258507,\n"," 0.013500300529397052,\n"," 0.02355547712461918,\n"," 0.0822110448065574,\n"," 0.004090262615711162,\n"," 0.019084314267249824,\n"," 0.0026653514492397637,\n"," 0.002581807272100706,\n"," 0.010777957851497216,\n"," 0.039789022698576655,\n"," 1.7355928229654318,\n"," 0.0015814408168602536,\n"," 0.0016489053002162965,\n"," 0.11745838435887575,\n"," 0.16952517788304852,\n"," 0.03612394890079164,\n"," 0.007975984431927966,\n"," 0.03751199413809367,\n"," 0.2796692663817038,\n"," 7.698716548000722e-06,\n"," 0.45399169910391296,\n"," 0.042277970777762096,\n"," 0.4353778074092262,\n"," 0.044904827755137905,\n"," 0.030973126590428002,\n"," 0.050299946972488616,\n"," 0.017782941642860324,\n"," 0.29568856650146325,\n"," 0.08951012525473097,\n"," 0.018128056104686204,\n"," 0.09975773689522827,\n"," 0.029791991131017957,\n"," 0.030880160611470163,\n"," 0.007161252702851296,\n"," 0.5555335009796438,\n"," 0.3620663966647567,\n"," 0.032256616147524245,\n"," 0.14815011323214033,\n"," 0.09320343447799011,\n"," 0.001865635244493165,\n"," 0.06781555226851269,\n"," 0.5030085268780163,\n"," 0.3470218253936693,\n"," 0.015505490239904095,\n"," 0.002044589447252591,\n"," 1.272134089810244,\n"," 0.0054258504126572715,\n"," 0.014863327388928663,\n"," 0.010626578423921534,\n"," 0.23662692957225306,\n"," 0.009386778786192002,\n"," 0.270130371339498,\n"," 0.07871971079815868,\n"," 0.537504957260258,\n"," 0.06676945105411175,\n"," 0.1639908801285024,\n"," 0.1283072237120611,\n"," 0.6713146650645121,\n"," 0.0003203491930981686,\n"," 0.067629142793819,\n"," 0.17867837131222794,\n"," 0.28092357439450855,\n"," 0.23761055539751236,\n"," 1.5109415183820263,\n"," 0.41547655779360193,\n"," 0.11718997481928677,\n"," 0.08452169168462922,\n"," 0.04227396413935013,\n"," 0.5704055015788019,\n"," 0.12084477762815593,\n"," 0.09398203809169936,\n"," 0.3185228492322896,\n"," 0.12828873837341795,\n"," 1.3843189563912188,\n"," 0.026028906767445858,\n"," 0.022537505555817752,\n"," 0.3583917006316415,\n"," 0.015491600350221577,\n"," 0.24958468249802648,\n"," 0.017478760795723924,\n"," 0.003924101803660172,\n"," 0.1783088870391544,\n"," 0.44573143732243276,\n"," 1.0572914391983346,\n"," 0.431984833170046,\n"," 0.006794201659530312,\n"," 0.026545699489495842,\n"," 0.07517542792588428,\n"," 0.03819517477286793,\n"," 0.12936438014350138,\n"," 0.013804645922445012,\n"," 0.25728019769283544,\n"," 0.07327850497338136,\n"," 0.06445398142713363,\n"," 0.618126643862537,\n"," 0.011926395642201066,\n"," 0.05935685587449942,\n"," 0.04338478068247212,\n"," 0.30123780059735483,\n"," 0.00032355924399781036,\n"," 0.0010016365434192501,\n"," 0.006515984051902021,\n"," 0.2885543390823384,\n"," 0.07461677380987848,\n"," 0.0008662075562152929,\n"," 0.0959828969739974,\n"," 0.00351990066960093,\n"," 0.2700111608937127,\n"," 0.009027563669172456,\n"," 0.024132432192977865,\n"," 0.1524178858836061,\n"," 1.6851268060622084,\n"," 0.009006548913095224,\n"," 3.3845367623104286e-05,\n"," 0.03228992951323355,\n"," 0.017457638911653042,\n"," 0.0002595913750568604,\n"," 0.3674359473176616,\n"," 0.041817384272097406,\n"," 0.0006286442632295433,\n"," 0.06918412146687325,\n"," 0.06061023380186,\n"," 0.04246897754221523,\n"," 0.09552767451806633,\n"," 0.3919382453371407,\n"," 0.02366421553270749,\n"," 0.3025682721466076,\n"," 0.001191175811809189,\n"," 0.000682482552439829,\n"," 0.06901852767998544,\n"," 2.0431944734838234,\n"," 0.10993882253472698,\n"," 2.119725533041869,\n"," 0.012163988818690437,\n"," 0.04027935504931889,\n"," 0.03971221862411097,\n"," 0.068159725515032,\n"," 0.17910998525810476,\n"," 0.3184358485899307,\n"," 0.8107401261782502,\n"," 2.4388534423348154,\n"," 0.33625345557237163,\n"," 0.06707445292689751,\n"," 6.887613730048384,\n"," 0.10598698836021263,\n"," 0.027861700720471862,\n"," 0.08256777181524119,\n"," 0.03806048492310635,\n"," 0.0007601285245878997,\n"," 8.862753350249427e-05,\n"," 0.039616132041819116,\n"," 0.025887507089678196,\n"," 0.0009137220809768432,\n"," 0.005989218890805941,\n"," 0.030299436636213303,\n"," 0.38727718615197493,\n"," 0.28190042181739466,\n"," 0.35337088427285407,\n"," 0.015388731348309127,\n"," 0.008989262700634618,\n"," 0.015078760956704037,\n"," 0.33043514986235095,\n"," 0.024270725362395218,\n"," 1.9823149160355244e-05,\n"," 0.3936486834416981,\n"," 0.02673265874317496,\n"," 0.19204578442050094,\n"," 0.0769072358292979,\n"," 0.020349221899686774,\n"," 0.36337809376781494,\n"," 0.8228784583410401,\n"," 0.04430466160013606,\n"," 0.1826399210002038,\n"," 0.012412223287335447,\n"," 0.2800049787646636,\n"," 0.03312341901973568,\n"," 0.011162577423257403,\n"," 0.09959551209287291,\n"," 0.2929840044132327,\n"," 0.11513936630577189,\n"," 0.35321948308103407,\n"," 0.2651520471182964,\n"," 0.0038891532265667864,\n"," 0.0572656753076076,\n"," 0.029834298540104387,\n"," 0.09454508105378039,\n"," 0.01060453334364523,\n"," 0.028940235004715428,\n"," 0.11821002134677283,\n"," 0.06287599012435695,\n"," 0.4101069198828748,\n"," 0.06630271862810418,\n"," 0.6652287005807443,\n"," 0.21297957786520547,\n"," 0.04659575581426854,\n"," 0.18067384892911872,\n"," 0.031374825665317095,\n"," 0.0019850255615634525,\n"," 0.09590805609736698,\n"," 0.0261465863419139,\n"," 0.32560804678426347,\n"," 0.4310813384283924,\n"," 0.003128202782986947,\n"," 0.17352268199556908,\n"," 0.02223928914020827,\n"," 0.0014240129566857513,\n"," 0.00260644224879461,\n"," 0.007086529615155683,\n"," 0.04062011565606767,\n"," 0.03757557075037296,\n"," 0.3048553278156318,\n"," 0.1578501922457783,\n"," 0.021441413257751916,\n"," 0.03279807700576118,\n"," 1.2436522882712095,\n"," 0.07613637231063662,\n"," 0.13993184907903208,\n"," 0.00038441169339296997,\n"," 0.5128675251776087,\n"," 0.3913702346156453,\n"," 0.011592684611667901,\n"," 1.8034030005316284,\n"," 0.7537189086548168,\n"," 0.12849205480448078,\n"," 0.0038337676891955315,\n"," 0.11242639359088952,\n"," 0.09673023448619829,\n"," 0.03170297610622929,\n"," 0.006888926355141805,\n"," 0.02266957000851032,\n"," 0.0378450682788329,\n"," 0.007878147739974138,\n"," 0.0337941514986216,\n"," 0.15429250531914665,\n"," 0.00042357000608395816,\n"," 0.18537535749486736,\n"," 1.331536588846646,\n"," 0.0033545256203487647,\n"," 1.5970173701081547,\n"," 0.010092729472147765,\n"," 0.04125894603458966,\n"," 0.008148786130733313,\n"," 1.4969603052131146,\n"," 2.2341610164696974,\n"," 0.03627964079269504,\n"," 0.5303861823349647,\n"," 0.3233907595449635,\n"," 0.06528121769236132,\n"," 0.01691541234258861,\n"," 0.0024626851810082557,\n"," 0.019041830322597826,\n"," 0.03554884753810945,\n"," 0.03906060023818274,\n"," 0.0010425736486021418,\n"," 0.36836100364580915,\n"," 0.25213514277930227,\n"," 0.5403784799059614,\n"," 0.0991052838299962,\n"," 0.03149906186667835,\n"," 0.27813778779303183,\n"," 1.333036440469627,\n"," 0.07854514382552912,\n"," 0.32347118962987553,\n"," 0.08946487115447023,\n"," 0.0032222941406412136,\n"," 0.035801022564941075,\n"," 0.002158712351054689,\n"," 0.13121057749159726,\n"," 0.09247097373896433,\n"," 0.12688008280014926,\n"," 0.5734931734337056,\n"," 0.26424947638034085,\n"," 0.00038001853007384867,\n"," 0.6318478389249979,\n"," 0.6934836158531746,\n"," 0.30093021073713555,\n"," 0.1364134914101779,\n"," 1.55112511154556,\n"," 0.03531574436706256,\n"," 0.17587488888132036,\n"," 0.5029865112421329,\n"," 0.20793337039318244,\n"," 0.24652173803592262,\n"," 0.0005235432066588818,\n"," 0.00034784907169297237,\n"," 0.01784408352303583,\n"," 0.04765942203669336,\n"," 1.4755388923698063,\n"," 0.02678435676821319,\n"," 0.006829036243939722,\n"," 0.0009569054277188163,\n"," 0.0019043234507710235,\n"," 0.10967955425712354,\n"," 0.1961754487001756,\n"," 0.8454110963294611,\n"," 0.00945356120883231,\n"," 0.021162873243226677,\n"," 0.04652457715187082,\n"," 0.14774467607775754,\n"," 0.15487836202052935,\n"," 0.0012429380852548213,\n"," 0.0021674367902679414,\n"," 0.07853458225723606,\n"," 0.02917494444853617,\n"," 0.018786481528574202,\n"," 0.03419769444525779,\n"," 0.24758704376198012,\n"," 0.11523470411887156,\n"," 0.038178357584139615,\n"," 0.0674166225241118,\n"," 0.1208315524785174,\n"," 0.08455180931182353,\n"," 0.11748496911596734,\n"," 0.022158468114007953,\n"," 0.13355688812692798,\n"," 0.441462659672519,\n"," 0.14261855791730657,\n"," 0.004173048688102784,\n"," 0.18315236714918226,\n"," 0.004894556619986118,\n"," 0.23648892948831057,\n"," 0.002032521262043334,\n"," 0.018725056332408174,\n"," 3.5937511840753148e-09,\n"," 0.44357184821738893,\n"," 0.04361733649808487,\n"," 0.2573029044139685,\n"," 0.07110382690561755,\n"," 0.011947627806607599,\n"," 1.6158461957598964,\n"," 0.25454441010687345,\n"," 1.520966587295436,\n"," 0.13216950894592303,\n"," 0.5892850299571484,\n"," 0.19421803638051285,\n"," 0.23056544424645103,\n"," 0.3001286976196997,\n"," 0.06992782639289745,\n"," 0.10173681230428551,\n"," 1.6967944781639763,\n"," 0.06308521814870612,\n"," 0.22664020034598967,\n"," 0.000680473463823389,\n"," 0.5028003287930845,\n"," 0.5702413842192382,\n"," 0.1491011139768843,\n"," 0.0992309849532853,\n"," 0.02832489083025317,\n"," 0.17839728211995062,\n"," 0.6630928072149787,\n"," 1.3963927537563112,\n"," 0.02379688069481347,\n"," 0.0003378836426157823,\n"," 0.2615859461255803,\n"," 0.03735173009828916,\n"," 2.8744692404997125,\n"," 0.0026120961017575216,\n"," 0.002364590593951572,\n"," 0.2838586386938471,\n"," 1.702053151392753,\n"," 0.02287141097517455,\n"," 0.008752477054357099,\n"," 0.521042431203294,\n"," 0.0281727669447681,\n"," 0.0444979619505553,\n"," 0.00539429669380689,\n"," 0.01925120265988704,\n"," 0.03728247931024496,\n"," 0.09573724634577059,\n"," 0.19199730915678592,\n"," 0.4064338012899945,\n"," 0.0030619502239089473,\n"," 0.10085327847375912,\n"," 0.1435920487905427,\n"," 0.39534510859185396,\n"," 0.03269725571969062,\n"," 0.1468691647234108,\n"," 0.10031650617036143,\n"," 0.35774139422641743,\n"," 0.00391768044879671,\n"," 0.0012152665298767885,\n"," 0.06136748466673773,\n"," 0.03050875925914717,\n"," 0.11535831979580416,\n"," 0.08593720976189395,\n"," 0.27499585010605176,\n"," 0.5097065650398054,\n"," 0.16594238627522448,\n"," 0.1969923755808926,\n"," 0.299366019405557,\n"," 0.01447065216696584,\n"," 0.12122106949016573,\n"," 0.0013012936736220382,\n"," 0.09042929670140032,\n"," 0.8550116205438024,\n"," 0.04912545098042773,\n"," 0.01690247516423184,\n"," 2.6753235097045596,\n"," 0.01442908446178986,\n"," 0.09638431817225566,\n"," 0.2679982267020694,\n"," 0.011835502181724952,\n"," 0.062394618096756234,\n"," 0.16858670469019443,\n"," 0.00234152636912553,\n"," 0.38028266655782966,\n"," 0.03469388670543094,\n"," 0.21031271766032078,\n"," 0.26912458041174764,\n"," 0.3914104927313504,\n"," 0.013786527751147894,\n"," 0.3087267630471333,\n"," 0.04914584538937413,\n"," 0.6757243828196338,\n"," 0.29960705631156503,\n"," 0.2852956229652206,\n"," 0.17443914271021282,\n"," 0.03295349280572744,\n"," 0.01597398926925893,\n"," 0.06813993200804283,\n"," 0.019978478502932263,\n"," 0.0016204918988897534,\n"," 0.02636892361582087,\n"," 1.5645666528579685,\n"," 1.7391399020227134,\n"," 0.0059667091544437344,\n"," 0.6304783929647021,\n"," 0.05808062968057381,\n"," 0.06422288940224243,\n"," 0.33243070673871794,\n"," 2.3502284563904037,\n"," 0.007914381034388393,\n"," 0.42972422260577736,\n"," 0.40937410292072995,\n"," 1.468455865344114,\n"," 0.00041622265707053167,\n"," 4.22917115184712e-05,\n"," 0.059132850378834485,\n"," 2.552969862288801,\n"," 0.27948629558687516,\n"," 0.25203542800443557,\n"," 0.0818584514465897,\n"," 0.009304235735391046,\n"," 2.775730250220948,\n"," 0.07111508368622711,\n"," 0.23342132707405172,\n"," 0.031615095423033424,\n"," 0.09640175419614237,\n"," 0.08447071798436265,\n"," 0.17349011459081395,\n"," 0.1859921951603509,\n"," 1.8602400533438537,\n"," 0.006282226990118077,\n"," 0.32652797650806453,\n"," 0.0218689424825972,\n"," 0.07717380064110295,\n"," 0.15874984712287857,\n"," 0.09150353704388192,\n"," 0.19027023127809292,\n"," 0.8707412375338826,\n"," 0.005093471596313761,\n"," 0.14534026705156416,\n"," 0.05940198728534728,\n"," 0.013968163784788695,\n"," 0.02098222956697717,\n"," 0.6355932188007014,\n"," 0.010511251959461919,\n"," 0.011031939609120753,\n"," 0.6543925721710795,\n"," 0.7761595961557941,\n"," 0.1738892594298542,\n"," 4.519316259286311e-05,\n"," 0.10050529743230877,\n"," 0.3331529348138113,\n"," 0.0002108360114705808,\n"," 0.05801172274386686,\n"," 3.0047281526146867,\n"," 0.012481311548096435,\n"," 0.22092108556580736,\n"," 0.06934449231543045,\n"," 0.07684260566545227,\n"," 0.037690809914357984,\n"," 0.41072523850926856,\n"," 0.19825107697567038,\n"," 0.016570631120942553,\n"," 0.011838584432290173,\n"," 0.005296058777074603,\n"," 0.009670607725904478,\n"," 0.010302170907675953,\n"," 0.09246988832930524,\n"," 0.08947627854416632,\n"," 0.05077799629168545,\n"," 0.016216126561133613,\n"," 0.04520437020921838,\n"," 0.030550179971877322,\n"," 0.0031986811437645702,\n"," 0.06706635430689185,\n"," 0.0025591110934251923,\n"," 0.02633540827139632,\n"," 1.563791309073163,\n"," 0.03271781747216057,\n"," 0.08700443141065724,\n"," 0.09830397329429817,\n"," 0.0425742095134489,\n"," 0.00834227831750764,\n"," 0.00785556128822182,\n"," 0.002873275748223383,\n"," 0.6599311589215803,\n"," 0.041068080285676864,\n"," 0.5093926731839813,\n"," 0.023488565664393363,\n"," 0.008647090421323111,\n"," 0.024810855958236365,\n"," 0.00986234288465882,\n"," 0.029445645629534056,\n"," 0.7310561549459623,\n"," 0.6166673314906954,\n"," 0.0872441959409618,\n"," 0.11644082549017949,\n"," 0.00023165234146375346,\n"," 0.024067044501856982,\n"," 0.023113484463776167,\n"," 0.00040874584388096653,\n"," 0.07737929843569331,\n"," 0.10891896875380797,\n"," 4.069455793365419,\n"," 0.05338304671599583,\n"," 0.20492412263397616,\n"," 0.10497205893541871,\n"," 0.0001341061204752044,\n"," 0.021067319934167727,\n"," 0.12087707184925547,\n"," 0.03772852408770606,\n"," 0.003217101769292977,\n"," 0.08613711959217313,\n"," 0.001807040260111148,\n"," 0.5702908690184156,\n"," 0.032392129066453675,\n"," 0.06935560426451608,\n"," 0.03269883364886755,\n"," 0.8623444466319806,\n"," 0.0006414899253474001,\n"," 0.03762110971826934,\n"," 0.002746236292666983,\n"," 0.00888835707246021,\n"," 0.8092786339498809,\n"," 0.030786578878109053,\n"," 0.6265682159328178,\n"," 0.8524030805307184,\n"," 1.8546266040456136e-05,\n"," 0.4106376730884696,\n"," 0.010386345253930016,\n"," 0.1701276716452222,\n"," 0.8591225925480187,\n"," 0.32883312663615855,\n"," 0.004924808786663168,\n"," 0.00420607614531472,\n"," 0.07803702926047325,\n"," 0.3900156946877713,\n"," 0.0747384353220262,\n"," 0.008839539458424707,\n"," 0.003595816545928762,\n"," 0.21051326249389019,\n"," 0.37668433841184923,\n"," 0.6922646779579592,\n"," 0.2593799666802973,\n"," 0.23532132336623857,\n"," 0.004726900037053041,\n"," 0.05798041450717528,\n"," 0.5215181129076687,\n"," 0.7382624833353592,\n"," 0.01322047565829607,\n"," 0.01409737837673594,\n"," 0.05497185171027926,\n"," 2.282747750104232,\n"," 1.4821985730805591,\n"," 0.02973900991755468,\n"," 0.2081918061958403,\n"," 0.0006537328542691664,\n"," 0.16386774196018328,\n"," 0.0005559033441440886,\n"," 0.001984946087164694,\n"," 0.19201679423493537,\n"," 0.6606690402360405,\n"," 0.02664732447565805,\n"," 1.0258547121750257,\n"," 7.718186566062201,\n"," 0.40991544791860857,\n"," 0.16112496541153723,\n"," 0.0956937106899223,\n"," 0.0053204224660598,\n"," 0.060475300962046195,\n"," 0.5497540897331608,\n"," 0.055827902537757684,\n"," 0.02103123919627637,\n"," 0.1855266159009148,\n"," 0.059107939502110435,\n"," 0.013265465288505366,\n"," 0.2518797674440273,\n"," 0.050001027538605115,\n"," 0.031952241491076665,\n"," 0.04049209774892573,\n"," 0.039837389677702555,\n"," 0.42083128097476447,\n"," 0.27904734017934696,\n"," 0.055752963579391634,\n"," 0.0035121793002846026,\n"," 2.070356657311945,\n"," 1.45529508609191,\n"," 0.15817230547222486,\n"," 0.11565070171339217,\n"," 0.03651666885522881,\n"," 0.2436197874253914,\n"," 2.716764295231569,\n"," 0.020098693198693316,\n"," 0.038816047396321836,\n"," 0.04692988176512686,\n"," 0.0025576882334221295,\n"," 0.14625136356135973,\n"," 0.09896112895632772,\n"," 7.938634021409902,\n"," 0.3751273228217189,\n"," 0.27324893597133537,\n"," 0.05672821241115743,\n"," 0.06097743124979409,\n"," 1.3342978416745395,\n"," 0.024953902998479514,\n"," 0.056165275659066184,\n"," 0.12997529365089422,\n"," 0.04450143879105576,\n"," 0.09883110561709121,\n"," 0.004036150064458981,\n"," 0.1378381962315465,\n"," 0.4748930583216906,\n"," 0.3974379533685486,\n"," 0.26119645755949844,\n"," 0.056471206719727336,\n"," 0.020970615049054845,\n"," 0.13360730695083542,\n"," 0.0005126259454648893,\n"," 0.3513287541226286,\n"," 0.004380150136741297,\n"," 0.09321636987019939,\n"," 0.285248988465244,\n"," 0.7636968301307014,\n"," 0.22914980300378415,\n"," 0.24904192959479773,\n"," 0.041055722859334384,\n"," 0.7846947790157106,\n"," 0.021399400235411502,\n"," 0.14472188305220585,\n"," 0.05694221469236416,\n"," 0.1610456730564016,\n"," 0.23747625037283088,\n"," 0.22431649930923622,\n"," 0.05976239593171798,\n"," 0.5205385347706135,\n"," 0.016653179210332263,\n"," 0.02522514199678079,\n"," 0.019019212111567735,\n"," 0.03295950632915799,\n"," 0.06996248001035427,\n"," 0.0011291379208465066,\n"," 0.18384716609896085,\n"," 0.04898977591625479,\n"," 0.6160949779132344,\n"," 0.596063217796391,\n"," 2.422469516453144,\n"," 2.4564829428097403,\n"," 0.0005589884307941905,\n"," 0.3192338598939563,\n"," 0.01828503916096173,\n"," 0.29377942529286827,\n"," 0.09445036145554238,\n"," 0.00827479480654623,\n"," 0.06097985228555738,\n"," 0.3007736210400106,\n"," 0.016281233370157806,\n"," 0.2065666015528903,\n"," 0.0032118166400293613,\n"," 0.6019633858480608,\n"," 0.525055611843591,\n"," 0.12630964817410525,\n"," 0.008582798746076365,\n"," 0.48669773546602724,\n"," 0.011639107477549886,\n"," 0.40324404734617697,\n"," 0.9396495157726112,\n"," 0.4721671250989814,\n"," 0.186549139769767,\n"," 0.0006799304250902749,\n"," 0.26002422259511865,\n"," 0.25917663025426557,\n"," 0.0003488482701210782,\n"," 0.10089919913454543,\n"," 0.00279160889885173,\n"," 3.6275960370580536,\n"," 0.021855538755495896,\n"," 0.45493971579846476,\n"," 4.882221515803811e-05,\n"," 0.3106787331339036,\n"," 0.6000537867965343,\n"," 0.673668079593643,\n"," 0.7415241071944193,\n"," 0.07124136288556389,\n"," 0.7001882451337356,\n"," 0.10675303469340056,\n"," 0.0039861906393688275,\n"," 0.35261477134075253,\n"," 0.3839851830433463,\n"," 0.03812541606082502,\n"," 0.02097087390141458,\n"," 0.004307704360593931,\n"," 1.3590484272764976,\n"," ...]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["import numpy as np\n","mse_otro = np.array(lista).mean()\n","print(round(mse_otro,3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6pZpzC_Q_15I","executionInfo":{"status":"ok","timestamp":1710554643054,"user_tz":300,"elapsed":193,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"c74eb023-04b1-4883-d4d4-014509cb9c48"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["0.329\n"]}]},{"cell_type":"code","source":["def mse_manual (y_prueba, y_predichos):\n","  error = y_prueba - y_predichos\n","  lista = [valor**2 for valor in error]\n","  mse = np.array(lista).mean()\n","  return round(mse,3)\n","\n","mse_manual(y_test,y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1b1lkQZ8Bdpv","executionInfo":{"status":"ok","timestamp":1710555197858,"user_tz":300,"elapsed":215,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"aab16bf1-5234-462f-a685-41be8d75e495"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.329"]},"metadata":{},"execution_count":33}]}]}