{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+vLmX6IzHxTDTSxorftBI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pi096DV1bH0X","executionInfo":{"status":"ok","timestamp":1715387878490,"user_tz":300,"elapsed":219,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"d4e37502-fc7c-4d9d-876b-e5c1822244f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2]]\n","rellena = \n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]]\n"]}],"source":["import keras\n","\n","frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longitud uniforme\n","relleno = keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena = \\n', relleno)"]},{"cell_type":"markdown","source":["Cuando el número de palabras en la frase es mayor al parámetro"],"metadata":{"id":"2meAq79vHwTC"}},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longitud uniforme\n","relleno = keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena = \\n', relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTQlv78YH4Rj","executionInfo":{"status":"ok","timestamp":1715387883696,"user_tz":300,"elapsed":262,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"7961554b-e109-4357-8148-a95bdac02322"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2], [7, 8, 9]]\n","rellena = \n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]\n"," [0 0 7 8 9]]\n"]}]},{"cell_type":"markdown","source":["Cuando el número de palabras en las frase es mayor al parámetro num_words ademas incluimos el parámetro OVV"],"metadata":{"id":"IKoabui5Iomh"}},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10,\n","                                               oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('word_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longitud uniforme\n","relleno = keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena = \\n', relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQQjvYXAI01I","executionInfo":{"status":"ok","timestamp":1715387889644,"user_tz":300,"elapsed":213,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"5aea8e47-fd78-4d45-cc7c-2965b2ce3c0d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","rellena = \n"," [[0 0 0 2 3]\n"," [0 0 2 4 5]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]},{"cell_type":"markdown","source":["Probemos los parámetros padding y truncating de la función pad_sequences en Keras se utilizan para controlar el relleno y el truncamiento de las secuencias derante el preprocesamiento de los datos"],"metadata":{"id":"Sfyd6EUFKSbP"}},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=10,\n","                                               oov_token = '<OOV>')\n","tokenizer.fit_on_texts(frases)\n","word_index = tokenizer.word_index\n","print('\\nword_index =', word_index)\n","\n","# Generación de secuencia tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =', secuencias)\n","\n","# Rellena las secuencias a una longitud uniforme\n","relleno = keras.preprocessing.sequence.pad_sequences(secuencias,\n","                                                     padding = 'post',\n","                                                     truncating = 'post')\n","print('rellena = \\n', relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8dQS49GKkp2","executionInfo":{"status":"ok","timestamp":1715387895005,"user_tz":300,"elapsed":209,"user":{"displayName":"Cristina Ortega","userId":"10739822531022913396"}},"outputId":"f56d46d0-4757-4791-cb0b-1aebb58a6b76"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","rellena = \n"," [[2 3 0 0 0]\n"," [2 4 5 0 0]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]}]}